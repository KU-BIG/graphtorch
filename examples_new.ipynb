{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphtorch import SparseMatrix, SparseModel\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import random\n",
    "import graphtorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example 1__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.array([[0,2,0,0,2,0,0,0,0,0],\n",
    "                [2,0,2,0,0,0,0,0,0,0],\n",
    "                [0,2,0,2,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,1,1,0,0,0],\n",
    "                [0,0,0,0,0,0,1,1,0,0],\n",
    "                [0,0,0,0,0,0,0,0,0,3],\n",
    "                [0,0,0,0,0,0,0,0,3,0]])   \n",
    "in_dim = 5   \n",
    "out_dim = 2  \n",
    "mat_wann1 = SparseMatrix(mat1, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__three methods to change architecture__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_activation(SparseMatrix, activations):\n",
    "    \n",
    "    in_dim = SparseMatrix.in_dim\n",
    "    out_dim = SparseMatrix.out_dim\n",
    "    mat = SparseMatrix.mat  \n",
    "    indices = (mat != 0) #0이 아닌 index들만 activation 바꿀 수 있으니까 true false matrix 받기   \n",
    "    possible = [] # activation을 바꿀 수 있는 index 리스트. [[0, 0], [1, 3], [1, 5]] 이런 식으로 나옴 \n",
    "    \n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            if indices[i,j] == True:\n",
    "                possible += [[i,j]]\n",
    "    \n",
    "    ran_idx = random.choice(possible) # [1,5] 이런 식으로 나오는데     \n",
    "    \n",
    "    #activations = [1, 2, 3] #linear, relu, sigmoid   \n",
    "    ran_activation = random.choice(activations)\n",
    "    \n",
    "    #change activation\n",
    "    mat[ran_idx[0],ran_idx[1]] = ran_activation \n",
    "    \n",
    "    new_SparseMatrix = graphtorch.SparseMatrix(mat, in_dim, out_dim) #이렇게 해야 callable 하더라고.. \n",
    "    \n",
    "\n",
    "    return new_SparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_connection(SparseMatrix):\n",
    "    \n",
    "    mat = SparseMatrix.mat \n",
    "    in_dim = SparseMatrix.in_dim\n",
    "    out_dim = SparseMatrix.out_dim  \n",
    "    hidden_dim = SparseMatrix.hidden_dim \n",
    "    \n",
    "    #random 하게 connection 추가 \n",
    "    # 같은 layer끼리는 connection x -> hidden layer끼리만 봐주면 됨 (input끼리 output끼리는 matrix에 없으니까) \n",
    "    # 0인 element 중에서 임의로 하나 바꾸기 -> 1,2,3 중에서(linear, relu, sigmoid)   \n",
    "    # 가능한 element 중에서 랜덤하게 하나의 element 뽑고, 거기서 또 (1,2,3) 중에서 랜덤하게 하나 뽑기  \n",
    "    \n",
    "    ###################\n",
    "    #hidden layer끼리 연결되어 있는 indices 들 뽑아내기\n",
    "    hidden_wise_indices = []\n",
    "    #print(hidden_dim[0])\n",
    "    row_start = 0\n",
    "    row_end = 0 + hidden_dim[0] - 1\n",
    "    col_start = in_dim \n",
    "    col_end = in_dim + hidden_dim[0] - 1 \n",
    "    #print(col_start, col_end)\n",
    "    \n",
    "    for hidden_count, current_hidden_dim in enumerate(hidden_dim): \n",
    "        #이전 laye의 dimension\n",
    "        for i in range(row_start, row_end + 1):\n",
    "            #print(col_start, col_end)\n",
    "            for j in range(col_start, col_end + 1):\n",
    "                \n",
    "                hidden_wise_indices += [[i,j]]\n",
    "        \n",
    "        row_start += current_hidden_dim  \n",
    "        col_start += current_hidden_dim \n",
    "        row_end += (current_hidden_dim - 1)  \n",
    "        col_end += (current_hidden_dim - 1)\n",
    "\n",
    "    ##################################    \n",
    "    #우선 0인 element들의 indices 받아오기   \n",
    "    indices = (mat == 0)     \n",
    "    zero_indices = [] # 0이 아닌 애들 indexes 우선 받아오기 \n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            if indices[i,j] == True:\n",
    "                zero_indices += [[i,j]]\n",
    "    \n",
    "    \n",
    "    #################################\n",
    "    # hidden_wise_indices에 해당되지 않는 zero_indices만이 possible에 포함될 수 있음  \n",
    "    # (zero_indices) and (! hidden_wise_indices)\n",
    "    # 같은 hidden layer끼리 있지 않은 인덱스들 \n",
    "    possible = []\n",
    "    possible = [value for value in zero_indices if value not in hidden_wise_indices]  \n",
    "    \n",
    "    \n",
    "    # 우선 랜덤하게 어떤 element 바꿀 지 선택\n",
    "    ran_idx = random.choice(possible)\n",
    "    # select random activation -> 안하고 바로 liear 로 세팅\n",
    "    #possible_activation = [1, 2, 3] #linear, relu, sigmoid   \n",
    "    #ran_activation = random.choice(possible_activation)\n",
    "    \n",
    "    mat[ran_idx[0], ran_idx[1]] = 1   \n",
    "    \n",
    "    new_SparseMatrix = graphtorch.SparseMatrix(mat, in_dim, out_dim)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    return new_SparseMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function 1 to add node\n",
    "def which_layer(total_dim, from_node_num, to_node_num):\n",
    "    #선택된 index가 어느 layer와 어느 layer에 해당하는지 list return \n",
    "    layer_idx = [0]*len(total_dim)\n",
    "    \n",
    "    #아래 for문으로 짜야할듯 \n",
    "    for idx in range(len(total_dim)):\n",
    "        if from_node_num <= sum(total_dim[0:(idx+1)]):  \n",
    "            layer_idx[idx] = 1\n",
    "            break\n",
    "            \n",
    "    for idx in range(len(total_dim)):  \n",
    "        if to_node_num <= sum(total_dim[0:(idx+1)]):\n",
    "            layer_idx[idx] = 1\n",
    "            break \n",
    "    \n",
    "    return layer_idx # [1,0,1,0,0] 형태로 return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function 2 to add node \n",
    "def extra_hidden_idx(layer_idx, total_dim):\n",
    "    idx_total = None\n",
    "    from_layer_idx = None\n",
    "    to_layer_idx = None\n",
    "    \n",
    "    for i in range(len(layer_idx)):\n",
    "        if from_layer_idx == None and layer_idx[i] == 1:\n",
    "            from_layer_idx = i\n",
    "        elif from_layer_idx != None and layer_idx[i] == 1:\n",
    "            to_layer_idx = i\n",
    "    print(\"from_layer_idx:{}\".format(from_layer_idx))\n",
    "    print(\"to_layer_idx:{}\".format(to_layer_idx))\n",
    "    \n",
    "    idx_total = sum(total_dim[0:(to_layer_idx)]) #이건 number은 아니고 0부터 시작하는 index \n",
    "    #num_total = idx_total + 1 #이건 앞에서부터의 순 노드 number(1부터 시작하는)\n",
    "    idx_hidden = idx_total - in_dim\n",
    "    #num_hidden = idx_hidden + 1\n",
    "    print(\"idx_total:{}\".format(idx_total))\n",
    "    #print(\"num_total:{}\".format(num_total))\n",
    "    print(\"idx_hidden:{}\".format(idx_hidden))\n",
    "    #print(\"num_hidden:{}\".format(num_hidden))\n",
    "    \n",
    "        \n",
    "    return idx_hidden # 몇 번째 hidden node에 삽입될 것인가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fuction 3 to add node \n",
    "def expand_dim_mat(mat, idx_hidden, in_dim):\n",
    "    col_idx = in_dim + idx_hidden\n",
    "    row_idx = idx_hidden\n",
    "    mat = np.insert(mat, row_idx, 0, axis = 0) #index, value to fill, x axis \n",
    "    mat = np.insert(mat, col_idx, 0, axis = 1) #index, value to fill, y axis \n",
    "\n",
    "    return mat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function 4 to add node \n",
    "def change_element(mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back):\n",
    "    #끊기는 connection의 새로운 matrix에서의 index \n",
    "    from_idx = ran_idx[1]  \n",
    "    to_idx = ran_idx[0] + 1 #hidden에 하나 추가가 되니까 하나 뒤로 밀림 \n",
    "    #삽입된 노드의 index\n",
    "    col_idx = in_dim + idx_hidden\n",
    "    row_idx = idx_hidden\n",
    "    print(\"from_idx:{}\".format(from_idx))\n",
    "    print(\"to_idx:{}\".format(to_idx))\n",
    "    print(\"col_idx:{}\".format(col_idx))\n",
    "    print(\"row_idx:{}\".format(row_idx))\n",
    "    \n",
    "    if front_or_back == 0: #front에 새로운 activation\n",
    "        mat[row_idx, from_idx] = ran_activation\n",
    "        mat[to_idx, col_idx] = original_activation\n",
    "    elif front_or_back == 1:\n",
    "        mat[row_idx, from_idx] = original_activation\n",
    "        mat[to_idx, col_idx] = ran_activation\n",
    "    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(SparseMatrix, activations):\n",
    "    \n",
    "    mat = SparseMatrix.mat \n",
    "    in_dim = SparseMatrix.in_dim\n",
    "    out_dim = SparseMatrix.out_dim  \n",
    "    hidden_dim = SparseMatrix.hidden_dim \n",
    "    \n",
    "    #이미 connection이 존재하는 부분 사이에 node 추가하기 \n",
    "    #원래 connection에 존재하던 activation 앞으로 넘길지 뒤로 넘길지 정하고, 나머지에는 랜덤하게 정해주기\n",
    "    #노드 추가해주면 dimension 추가해주고 connection 다시 계산해 줘야 하는데.. \n",
    "    \n",
    "    # connection이 존재하는 indices추출 \n",
    "    indices = (mat != 0)\n",
    "    possible = [] #node를 add할 수 있는 connection들 index 리스트 \n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            if indices[i,j] == True:\n",
    "                possible += [[i,j]]    \n",
    "    #가능한 인덱스 중에서 하나 선택 \n",
    "    ran_idx = random.choice(possible) #[i,j] 형태로 나옴 \n",
    "    #랜덤하게 정할 activation 뽑기  \n",
    "    possible_activation = [i for i in range(len(activations))]\n",
    "    ran_activation = random.choice(possible_activation[1:]) \n",
    "    #원래 connection에 존재하던 activation type 앞으로 할지 뒤로 할지\n",
    "    front = 0 \n",
    "    back = 1\n",
    "    front_or_back = random.choice([front, back]) \n",
    "    #선택된 index의 원래 actavation 가지고있기\n",
    "    original_activation = mat[ran_idx[0], ran_idx[1]] \n",
    "    #선택된 index의 element 값 0으로 만들어주기\n",
    "    mat[ran_idx[0], ran_idx[1]] = 0\n",
    "    \n",
    "    ## 몇번째 layer에 해당하는지만   \n",
    "    #그 layer 마지막에 추가해주기   \n",
    "    total_dim = [in_dim]\n",
    "    for i in range(len(hidden_dim)):   \n",
    "        total_dim += [hidden_dim[i]]\n",
    "    total_dim += [out_dim]  \n",
    "    \n",
    "    #선택된 index 사이에 추가할 노드가 몇번째 hidden layer의 몇번째 hidden node인지 알아내야 -> mat의 index로 나오도록 \n",
    "    # 선택된 index의 from = input + 그 이전의 hidden 노드 개수 \n",
    "    # 선택된 index의 to = input + 그 이전의 hidden 노드 개수 + 그 이전의 output 노드 개수 \n",
    "    # 우선 from이 앞에서부터 몇번째 노드인지 알아내기 (1부터 시작하는 node number)  \n",
    "    from_node_num = ran_idx[1] + 1 \n",
    "    to_node_num = in_dim + ran_idx[0] + 1 \n",
    "    layer_idx = which_layer(total_dim, from_node_num, to_node_num) #function 1\n",
    "    idx_hidden = extra_hidden_idx(layer_idx, total_dim) #function 2\n",
    "    \n",
    "    #matrix dimension 늘리기(앞의 index 활용해서 늘려야함)     \n",
    "    #추가 col idx = in_dim + idx_hidden\n",
    "    #추가 row idx = idx_hidden \n",
    "    mat = expand_dim_mat(mat, idx_hidden, in_dim) #function 3\n",
    "    \n",
    "    #element 값 할당하기  \n",
    "    #mat = change_element(mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back) #function 4  \n",
    "    \n",
    "    \n",
    "    #sparsematrix 객체 update\n",
    "    #new_SparseMatrix = graphtorch.SparseMatrix(mat, in_dim, out_dim) \n",
    "    \n",
    "    return mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__print 디버깅__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 0 2 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "new = add_connection(mat_wann1)   \n",
    "print(new.mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 2], [0, 3], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [1, 1], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [2, 0], [2, 2], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 7], [3, 8], [3, 9], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [4, 8], [4, 9], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 9]]\n"
     ]
    }
   ],
   "source": [
    "indices = (mat_wann1.mat == 0)\n",
    "possible = []\n",
    "for i in range(indices.shape[0]):\n",
    "    for j in range(indices.shape[1]):\n",
    "        if indices[i,j] == True:\n",
    "            possible += [[i,j]]\n",
    "\n",
    "            \n",
    "print(possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_activation = [1,2,3]\n",
    "random.choice(possible_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0]\n",
      " [0 0 0 2 0 1]]\n",
      "<class 'graphtorch.SparseMatrix'>\n"
     ]
    }
   ],
   "source": [
    "sparsemat = change_activation(mat_wann2)\n",
    "print(sparsemat.mat)\n",
    "print(type(sparsemat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curent_hidden_dim: 3\n",
      "row_start: 0\n",
      "row_end: 2\n",
      "col_start: 5\n",
      "col_end: 7\n",
      "curent_hidden_dim: 2\n",
      "row_start: 3\n",
      "row_end: 4\n",
      "col_start: 8\n",
      "col_end: 9\n"
     ]
    }
   ],
   "source": [
    "hidden_wise_indices = []\n",
    "row_start = 0\n",
    "row_end = 0 + mat_wann1.hidden_dim[0] - 1\n",
    "col_start = in_dim \n",
    "col_end = in_dim + mat_wann1.hidden_dim[0] - 1 \n",
    "    \n",
    "for hidden_count, current_hidden_dim in enumerate(mat_wann1.hidden_dim): \n",
    "    \n",
    "    #이전 laye의 dimension\n",
    "    \n",
    "    print(\"curent_hidden_dim: {}\".format(current_hidden_dim))\n",
    "    \n",
    "    print(\"row_start: {}\".format(row_start))\n",
    "    print(\"row_end: {}\".format(row_end))\n",
    "    print(\"col_start: {}\".format(col_start))\n",
    "    print(\"col_end: {}\".format(col_end))\n",
    "    for i in range(row_start, row_end + 1):\n",
    "        for j in range(col_start, col_end + 1):\n",
    "            hidden_wise_indices += [[i,j]]\n",
    "    \n",
    "    row_start += current_hidden_dim  \n",
    "    col_start += current_hidden_dim \n",
    "    row_end += (current_hidden_dim - 1)  \n",
    "    col_end += (current_hidden_dim - 1) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5], [0, 6], [0, 7], [1, 5], [1, 6], [1, 7], [2, 5], [2, 6], [2, 7], [3, 8], [3, 9], [4, 8], [4, 9]]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_wise_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0], [0, 2], [0, 3], [0, 8], [0, 9], [1, 1], [1, 3], [1, 4], [1, 8], [1, 9], [2, 0], [2, 2], [2, 4], [2, 8], [2, 9], [3, 0], [3, 1], [3, 2], [3, 3], [3, 4], [3, 7], [4, 0], [4, 1], [4, 2], [4, 3], [4, 4], [4, 5], [5, 0], [5, 1], [5, 2], [5, 3], [5, 4], [5, 5], [5, 6], [5, 7], [5, 8], [6, 0], [6, 1], [6, 2], [6, 3], [6, 4], [6, 5], [6, 6], [6, 7], [6, 9]]\n"
     ]
    }
   ],
   "source": [
    "lists = [value for value in possible if value not in hidden_wise_indices ]\n",
    "print(lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0]\n",
      " [0 0 0 3 0 1]]\n",
      "(2, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### test add dimension\n",
    "print(mat2)\n",
    "print(mat2.shape)\n",
    "idx = [1,3] \n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 3, 0, 1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "from_layer_idx:0\n",
      "to_layer_idx:1\n",
      "idx_total:5\n",
      "idx_hidden:0\n"
     ]
    }
   ],
   "source": [
    "## add node test\n",
    "activations = [None, None, nn.ReLU(), nn.Sigmoid()]\n",
    "print(mat_wann1.hidden_dim)\n",
    "mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back = add_node(mat_wann1, activations)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n",
      "5\n",
      "0\n",
      "[2, 1]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(mat)\n",
    "print(in_dim+idx_hidden)\n",
    "print(idx_hidden)\n",
    "print(ran_idx)\n",
    "print(ran_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_idx:1\n",
      "to_idx:3\n",
      "col_idx:5\n",
      "row_idx:0\n",
      "[[0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 2 0 2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "mat = change_element(mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 2 0 0 2 0 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 2 0 2 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3]\n",
      " [0 0 0 0 0 0 0 0 0 3 0]]\n"
     ]
    }
   ],
   "source": [
    "mat = expand_dim_mat(mat_wann1.mat, idx_hidden, in_dim)\n",
    "print(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ran_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-a181d86548e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mchange_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mran_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mran_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfront_or_back\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ran_idx' is not defined"
     ]
    }
   ],
   "source": [
    "change_element(mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_layer_idx:0\n",
      "to_layer_idx:1\n",
      "idx_total:5\n",
      "idx_hidden:0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-c84c76784635>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# total test for add node\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mactivations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msparse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat_wann1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-5c6b4e845ab7>\u001b[0m in \u001b[0;36madd_node\u001b[1;34m(SparseMatrix, activations)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;31m#element 값 할당하기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m     \u001b[0mmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchange_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mran_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mran_activation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfront_or_back\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#function 4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-149ee098a4b4>\u001b[0m in \u001b[0;36mchange_element\u001b[1;34m(mat, idx_hidden, ran_idx, in_dim, original_activation, ran_activation, front_or_back)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfront_or_back\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#front에 새로운 activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfrom_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mran_activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mmat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moriginal_activation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mfront_or_back\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "# total test for add node\n",
    "activations = [None, None, nn.ReLU(), nn.Sigmoid()]\n",
    "sparse = add_node(mat_wann1, activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 3 0 1]]\n"
     ]
    }
   ],
   "source": [
    "tmp = np.insert(mat2, 1, 0, axis = 0) #index, value to fill, axis \n",
    "print(tmp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "tmp = np.insert(tmp, 6, 0, axis = 1)\n",
    "print(tmp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "which_layer([5,8,9,10,11,3], 6, 17) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-38-2f292e6bc2b5>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-2f292e6bc2b5>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [0:10]\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
