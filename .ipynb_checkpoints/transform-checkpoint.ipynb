{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import torch  \n",
    "from torch import nn    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "# 전체 인접행렬 생성 \n",
    "\n",
    "mat = np.array([[0,0,1,0,0,0,0],[1,0,0,0,0,0,0],[0,1,1,0,0,0,0],[1,0,1,0,0,0,0],[0,1,0,0,0,0,0],[0,0,0,1,0,0,0],[0,0,0,1,1,0,0],[0,0,0,0,1,0,0],[0,0,0,0,0,0,1],[0,0,0,0,0,1,0]])\n",
    "print(mat)\n",
    "print(mat.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input layer랑 output layer만 있는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#아래 코드를 좀 변형하면 될듯? 아래 코드는 hidden layer가 없어서.. \n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, indices_mask):\n",
    "        \"\"\"\n",
    "       :param in_features: number of input features\n",
    "       :param out_features: number of output features\n",
    "       :param indices_mask: list of two lists containing indices for dimensions 0 and 1, used to create the mask  \n",
    "       \"\"\"\n",
    "        super(MaskedLinear, self).__init__()    \n",
    "        \n",
    " \n",
    "        def backward_hook(grad): \n",
    "            # Clone due to not being allowed to modify in-place gradients\n",
    "            out = grad.clone()  \n",
    "            out[self.mask] = 0    \n",
    "            return out \n",
    " \n",
    "        self.linear = nn.Linear(in_dim, out_dim)#.cuda()    \n",
    "        self.mask = torch.ones([out_dim, in_dim]).byte()#.cuda()\n",
    "        self.mask[indices_mask] = 0 # create mask\n",
    "        self.linear.weight.data[self.mask] = 0 # zero out bad weights\n",
    "        self.linear.weight.register_hook(backward_hook) # hook to zero out bad gradients\n",
    " \n",
    "    def forward(self, input):\n",
    "        return self.linear(input)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [1, 0, 1, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_mask = torch.ones([2,4]).byte() #[output_dim, input_dim] \n",
    "indices_mask[0,0] = 0  # [output_index, input_index]: gradient 0으로 해주고 싶은 부분 \n",
    "indices_mask[1,1] = 0 \n",
    "indices_mask[0,2] = 0\n",
    "indices_mask[0,3] = 0\n",
    "indices_mask[1,3] = 0\n",
    "indices_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedLinear(4, 2, indices_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.2785,  0.0000,  0.0000],\n",
       "        [-0.2201,  0.0000,  0.0551,  0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hidden layer까지 있는 경우  \n",
    "- adjacency matrix의 transpose 버전인 mat_mask를 통해서 hidden layer들의 dimension을 구한다 \n",
    "- 나머지 부분 바꿔서 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [1 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 1 0]]\n",
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "# 전체 인접행렬 생성 \n",
    "mat = np.array([[0,0,1,0,0,0,0],[1,0,0,0,0,0,0],[0,1,1,0,0,0,0],[1,0,1,0,0,0,0],[0,1,0,0,0,0,0],[0,0,0,1,0,0,0],[0,0,0,1,1,0,0],[0,0,0,0,1,0,0],[0,0,0,0,0,0,1],[0,0,0,0,0,1,0]])\n",
    "print(mat)\n",
    "print(mat.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 1 0 0 0 0 0]\n",
      " [1 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0]]\n",
      "(7, 10)\n"
     ]
    }
   ],
   "source": [
    "mat_mask = np.transpose(mat)\n",
    "print(mat_mask)\n",
    "print(mat_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]], dtype=torch.int32)\n",
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "mat_mask = torch.from_numpy(mat_mask)\n",
    "print(mat_mask)\n",
    "print(mat_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1, 0],\n",
       "        [0, 0, 1, 0, 1],\n",
       "        [1, 0, 1, 1, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_mask[0:3, 0:5] #index (0~2, 0~4) 표현할라면 이렇게.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1, dtype=torch.uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_mask[3,0:5].sum() == 0\n",
    "#이때 3이 첫번째 hidden layer의 dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer 추가한 버전\n",
    "#아래 코드를 좀 변형하면 될듯? 아래 코드는 hidden layer가 없어서.. \n",
    "class MaskedLinear(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, mat_mask):\n",
    "        \"\"\"\n",
    "       :param in_features: number of input features\n",
    "       :param out_features: number of output features\n",
    "       :param indices_mask: list of two lists containing indices for dimensions 0 and 1, used to create the mask  \n",
    "       \"\"\"\n",
    "        super(MaskedLinear, self).__init__()    \n",
    "        \n",
    "        def calculate_hidden_dim(in_dim, out_dim, mat_mask):\n",
    "            #hidden dim의 정보를 담은 list를 출력한다..?\n",
    "            hidden_dim_list = []\n",
    "            start_col_idx = 0 \n",
    "            finish_col_idx = in_dim - 1\n",
    "            \n",
    "            while(True):\n",
    "                if finish_col_idx >= mat_mask.shape[1]:\n",
    "                    break \n",
    "        \n",
    "                 for i in range(start_col_idx, len(mat_mask)): \n",
    "                    if (mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):\n",
    "                        hidden_dim_list += [i] \n",
    "                        start_col_idx = finish_col_idx + 1\n",
    "                        finish_col_idx += i \n",
    "                        break \n",
    "            \n",
    " \n",
    "        def backward_hook(grad): \n",
    "            # Clone due to not being allowed to modify in-place gradients\n",
    "            out = grad.clone()  \n",
    "            out[self.mask] = 0    \n",
    "            return out  \n",
    "        \n",
    "        #이부분 바꿔야함\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim1) \n",
    "        self.mask1 = torch.ones([hidden_dim1, in_dim]).byte()\n",
    "        self.mask1[mat_mask의 일부] = 0 # create mask\n",
    "        self.linear.weight.data[self.mask1] = 0 # zero out bad weights\n",
    "        self.linear.weight.register_hook(backward_hook) # hook to zero out bad gradients\n",
    " \n",
    "    #이부분 바꿔야함\n",
    "    def forward(self, input):  \n",
    "        return self.linear(input) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test 용 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = [] \n",
    "temp += [1]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp += [2]\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_mask.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim_list = []\n",
    "start_col_idx = 0 \n",
    "finish_col_idx = 4 # 5 - 1\n",
    "            \n",
    "while(True):\n",
    "    \n",
    "    if finish_col_idx >= mat_mask.shape[1]:\n",
    "        break\n",
    "        \n",
    "    for i in range(start_col_idx, len(mat_mask)): \n",
    "        if (mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):\n",
    "            hidden_dim = i - sum(hidden_dim_list)\n",
    "            hidden_dim_list += [hidden_dim] \n",
    "            start_col_idx = finish_col_idx + 1\n",
    "            finish_col_idx += i \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2]\n",
      "8\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(hidden_dim_list)\n",
    "print(start_col_idx)\n",
    "print(finish_col_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim_list = []\n",
    "sum(hidden_dim_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
