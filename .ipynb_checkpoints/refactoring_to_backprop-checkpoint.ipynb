{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMatrix():\n",
    "    \n",
    "    def __init__(self, mat, in_dim, out_dim):\n",
    "        # get when initialized\n",
    "        self.mat = mat  \n",
    "        self.in_dim = in_dim  \n",
    "        self.out_dim = out_dim  \n",
    "        \n",
    "        #calculate total number of hidden nodes\n",
    "        self.num_hidden_nodes = self.mat.shape[1] - self.in_dim   \n",
    "        \n",
    "        #calculate total number of connection in matrix \n",
    "        self.connection_count = np.count_nonzero(self.mat)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #when matrix has hidden layer  \n",
    "        if self.num_hidden_nodes == 1:  \n",
    "            self.hidden_dim = [1] \n",
    "        elif self.num_hidden_nodes == 0:\n",
    "            self.hidden_dim = []\n",
    "        else:\n",
    "            self.hidden_dim = self.get_hidden_dim()   \n",
    "            \n",
    "            \n",
    "    def get_hidden_dim(self):\n",
    "        in_dim = self.in_dim\n",
    "        out_dim = self.out_dim\n",
    "        mat_mask = self.mat\n",
    "        \n",
    "        hidden_dim_list = []\n",
    "        start_col_idx = 0\n",
    "        finish_col_idx = in_dim -1   \n",
    "        \n",
    "        while(True):\n",
    "            \n",
    "            if finish_col_idx >= mat_mask.shape[1]:   \n",
    "                print(finish_col_idx)\n",
    "                break  \n",
    "            \n",
    "            if ((mat_mask.shape[0] - sum(hidden_dim_list)) == out_dim):  #example4 해결\n",
    "                 break  #지금 hidden dimension들 합이랑 output dim 합이 row길이랑 같으면 더이상 탐색 필요 x\n",
    "            \n",
    "            for i in range(sum(hidden_dim_list), len(mat_mask)): #이부분이상한데..?   \n",
    "    \n",
    "                #밑에처럼 하면 example 2에서 오류가 남.\n",
    "                #skip connection에 대한 예외처리 해줘야 함   \n",
    "    \n",
    "                if(mat_mask[i,start_col_idx:(finish_col_idx + 1)].sum() == 0):   \n",
    "                \n",
    "                    hidden_dim = i - sum(hidden_dim_list)\n",
    "                    hidden_dim_list += [hidden_dim]\n",
    "                    start_col_idx = finish_col_idx + 1\n",
    "                    finish_col_idx += hidden_dim   \n",
    "                    break    \n",
    "                    \n",
    "        return hidden_dim_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseModel(nn.Module) : \n",
    "    def __init__(self, mat_wann, activations, constant_weight) : \n",
    "        super(SparseModel, self).__init__()\n",
    "        self.mat = mat_wann.mat\n",
    "        self.in_dim = mat_wann.in_dim  \n",
    "        self.out_dim = mat_wann.out_dim  \n",
    "        self.num_hidden_nodes = mat_wann.num_hidden_nodes\n",
    "        self.hidden_dim = mat_wann.hidden_dim\n",
    "        \n",
    "        self.activations = activations\n",
    "        self.constant_weight = constant_weight\n",
    "        \n",
    "        self.nodes = {}\n",
    "        '''\n",
    "        nodes라는 dictionary 안에 아래와 같이 저장됨\n",
    "        'hidden_1' : 해당 노드\n",
    "        'hidden_2' : 해당 노드\n",
    "        ...\n",
    "        'output_1' : 해당 output 노드, hidden node로부터 연결되어있음\n",
    "        'output_2' : 해당 output 노드, input node, hidden node로부터 연결되어있음\n",
    "        '''\n",
    "        #declare architecture template \n",
    "        self.connection_count = mat_wann.connection_count        \n",
    "        self.modules = []\n",
    "        for i in range(self.connection_count):\n",
    "            self.modules.append(nn.Linear(1, 1, bias = False))  \n",
    "        self.sequential = nn.Sequential(*self.modules)      \n",
    "            \n",
    "            \n",
    "        \n",
    "    def forward(self, x) : \n",
    "        \n",
    "        # hidden node가 한개라도 있을때\n",
    "        self.connect(x)\n",
    "        \n",
    "        # output은 반드시 있음\n",
    "        outputs = self.concat_output()\n",
    "        \n",
    "        return outputs, self.nodes\n",
    "    \n",
    "    \n",
    "    def concat_output(self) :\n",
    "        for idx_output_node in list(range(self.out_dim)) :\n",
    "            #print('output %d' %idx_output_node)\n",
    "            #print(self.nodes['output_%d'%idx_output_node])\n",
    "            if idx_output_node == 0 :\n",
    "                outputs = self.nodes['output_%d'%idx_output_node]\n",
    "            else : \n",
    "                outputs = torch.cat((outputs, self.nodes['output_%d'%idx_output_node]), 1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def wrap_activation(self, x, idx_activation, activations, constant_weight, total_count_connection) :    \n",
    "        if idx_activation == 0 :\n",
    "            assert True\n",
    "        elif idx_activation == 1 :\n",
    "            layer = self.modules[total_count_connection] #이부분을 modules에서 꺼내주는 걸로 바꿔야함 \n",
    "            layer.weight.data.fill_(constant_weight)\n",
    "            return layer(x)\n",
    "        else : \n",
    "            layer = self.modules[total_count_connection] #이부분을 modules에서 꺼내주는 걸로 바꿔야함 \n",
    "            layer.weight.data.fill_(constant_weight)\n",
    "            return activations[idx_activation](layer(x))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def connect(self, x) : \n",
    "        # input layer와 모든 이전 hidden layer를 탐색\n",
    "        # 그렇지 않으면 skip connection을 놓칠수 있음\n",
    "        # 모든 node와 connection은 dictionary self.nodes에 저장\n",
    "        #print(self.hidden_dim)\n",
    "        hidden_node_counts = 0\n",
    "        total_count_connection = 0\n",
    "        \n",
    "        #hidden 노드가 없어도 이 코드가 돌아가도록  \n",
    "        if self.num_hidden_nodes == 0:  \n",
    "            \n",
    "            ## input이랑 output만 이어주기\n",
    "            for idx_output_row in range(self.mat.shape[0]): \n",
    "                \n",
    "                connections_from_input = self.mat[idx_output_row,:]  \n",
    "                if connections_from_input.sum() != 0:  \n",
    "                    count_connection = 0 \n",
    "                    input_node = None\n",
    "                \n",
    "                    for idx_input_col, activation_type in enumerate(connections_from_input): \n",
    "                        \n",
    "                        if activation_type != 0 and count_connection == 0:  \n",
    "                            input_node = self.wrap_activation(x[:, idx_input_col].view(-1,1), activation_type, self.activations, self.constant_weight, total_count_connection)\n",
    "                            count_connection += 1\n",
    "                            total_count_conneciton += 1\n",
    "                        elif activation_type != 0 and count_connection != 0 :   \n",
    "                            new_node = None\n",
    "                            new_node = self.wrap_activation(x[:, idx_input_col].view(-1,1), activation_type, self.activations, self.constant_weight, total_count_connection)  \n",
    "                            count_connection += 1\n",
    "                            total_count_connection += 1\n",
    "                            input_node = input_node + new_node    \n",
    "                        \n",
    "                self.nodes['output_%d'%(idx_output_row)] = input_node  \n",
    "            \n",
    "            \n",
    "        \n",
    "        ############################### loop for hidden nodes + output nodes  \n",
    "        else:\n",
    "            \n",
    "            for idx_hidden_row in list(range(0, self.mat.shape[0])) :   \n",
    "                #connections_from_input = self.mat[idx_hidden_row, :self.in_dim]\n",
    "                connections_from_input = self.mat[idx_hidden_row, :]\n",
    "                #print('connection from input : ', connections_from_input)\n",
    "                if connections_from_input.sum() != 0 :  \n",
    "                    count_connection = 0   \n",
    "                    input_node = None   \n",
    "                    ############################# loop for input nodes\n",
    "                    for idx_input_col, activation_type in enumerate(connections_from_input) :\n",
    "                        #print('idx_input_col %s, activation_type %s' % (idx_input_col, activation_type))\n",
    "                        if activation_type != 0 and count_connection == 0:\n",
    "                            # x[sample index, positional index for input]\n",
    "                            #print('\\n**first input node')\n",
    "\n",
    "                            # 1) idx_input_col 이 input에서 오는 경우\n",
    "                            if idx_input_col < self.in_dim : \n",
    "                                input_node = self.wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, self.activations, self.constant_weight, total_count_connection)\n",
    "                            # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                            elif idx_input_col >= self.in_dim : \n",
    "                                input_node = self.wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, self.activations, self.constant_weight, total_count_connection)  \n",
    "\n",
    "                            #print(input_node)\n",
    "                            count_connection += 1\n",
    "                        elif activation_type != 0 and count_connection != 0 :\n",
    "                            #print('%s input node' % idx_input_col)\n",
    "                            # x[sample index, positional index for input]\n",
    "                            # torch.sum returns the addition of two tensors\n",
    "\n",
    "                            #print('\\n**input_node', input_node.shape)\n",
    "                            #print(input_node)\n",
    "\n",
    "                            #new_node = wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations)\n",
    "\n",
    "                            new_node = None\n",
    "                            # 1) idx_input_col 이 input에서 오는 경우\n",
    "                            if idx_input_col < self.in_dim : \n",
    "                                new_node = self.wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, self.activations, self.constant_weight, total_count_connection)\n",
    "                            # 2) idx_input_col이 hidden에서 오는 경우\n",
    "                            elif idx_input_col >= self.in_dim : \n",
    "                                new_node = self.wrap_activation(self.nodes['hidden_%d'%(idx_input_col-self.in_dim)], activation_type, self.activations, self.constant_weight, total_count_connection)\n",
    "\n",
    "\n",
    "\n",
    "                            #print('\\n**wrap_activation', new_node.shape)\n",
    "                            #print(new_node)\n",
    "                            input_node = input_node + new_node\n",
    "                            #print('\\n**sum', input_node.shape)\n",
    "                            #print(input_node)\n",
    "\n",
    "\n",
    "                            #input_node = torch.sum(input_node, wrap_activation(x[:, idx_input_col].view(-1, 1), activation_type, activations))\n",
    "                            count_connection += 1\n",
    "                # connect all input nodes to given hidden node\n",
    "                if idx_hidden_row < self.num_hidden_nodes : \n",
    "                    self.nodes['hidden_%d'%idx_hidden_row] = input_node    \n",
    "                else : \n",
    "                    self.nodes['output_%d'%(idx_hidden_row-self.num_hidden_nodes)] = input_node      \n",
    "            # sum all numbers of hidden nodes from this layer      \n",
    "            hidden_node_counts += 1     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = np.array([[0,2,0,0,2,0,0,0,0,0],\n",
    "                [2,0,2,0,0,0,0,0,0,0],\n",
    "                [0,2,0,2,0,0,0,0,0,0],\n",
    "                [0,0,0,0,0,1,1,0,0,0],\n",
    "                [0,0,0,0,0,0,1,1,0,0],\n",
    "                [0,0,0,0,0,0,0,0,0,3],\n",
    "                [0,0,0,0,0,0,0,0,3,0]])  \n",
    "in_dim = 5   \n",
    "out_dim = 2  \n",
    "mat_wann1 = SparseMatrix(mat1, in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat_wann1.connection_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "wrap_activation() missing 1 required positional argument: 'total_count_connection'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-e348d9343261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnumpy_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-ba031b7a9efd>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m# hidden node가 한개라도 있을때\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# output은 반드시 있음\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-ba031b7a9efd>\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m                             \u001b[1;31m# 1) idx_input_col 이 input에서 오는 경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0midx_input_col\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_dim\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m                                 \u001b[0minput_node\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrap_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_input_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m                             \u001b[1;31m# 2) idx_input_col이 hidden에서 오는 경우\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                             \u001b[1;32melif\u001b[0m \u001b[0midx_input_col\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_dim\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: wrap_activation() missing 1 required positional argument: 'total_count_connection'"
     ]
    }
   ],
   "source": [
    "activations = [None, None, nn.ReLU(), nn.Sigmoid()]  \n",
    "constant_weight = 1 \n",
    "model = SparseModel(mat_wann1, activations, constant_weight)\n",
    "\n",
    "numpy_input = np.array([[1,2,3,4,5],  \n",
    "                        [6,7,8,9,10],  \n",
    "                        [11,12,13,14,15]])      \n",
    "\n",
    "numpy_input = torch.from_numpy(numpy_input).float()  \n",
    "output, nodes = model(numpy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.connection_count   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of SparseModel(\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (4): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (5): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (6): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (7): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (8): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (9): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (10): Linear(in_features=1, out_features=1, bias=True)\n",
       "    (11): Linear(in_features=1, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
